## Machine-learned Intermediate Brain Language

Machine learning has proven to work well in the "blind" scenarios where no high level semantics is specified by a human. For example, given enough data about correspondence of sentences between two completely unknown human written languages, it can produce a meaningful translator between the languages.

Brain signals can be read with some precision today. Technology of reading is barebones, but it can and will improve. What is more unclear is how to interpret the signals: every human has their brains wired up somewhat uniquely. It's therefore more challenging to make sense of the signals than it is to read them.

What if we treat every human brain's signals as a separate language, unknown to both the machine and other humans? Now suppose we have 2 humans that can communicate to a machine translator if they are thinking about the same thing. At the end of the day, such ML system would be effectively mind-reading them, just understandable by other person only.

Now, let's add more humans. With 3, 4, and above humans participating in the context of the same ML system, what eventually is going to happen is an emergence of an intermediate language in the logical schemas of the ML. As the system expands, more and more people would be able to communicate with each other by simply thinking. The common language solidifies, and the machine may start learning to interact with it as well, effectively removing the needs for computer input devices.

Ideally, the system would be completely crowd-sourced. A product could come to the market in 2 variations:
  1. private communication device: locally-hosted and learning to exchange thoughts between limited number of humans
  2. public communication, where the data is exchanged with company-owned servers, where the common language is emerging
